{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment6.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/2kunal6/UniBonn/blob/master/cuda_lab/6/Assignment6.ipynb",
      "authorship_tag": "ABX9TyPxN0MeyQyMJL8lef5Qi2kK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8847649eb0eb44fdab9258cb9ca179f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d2c305d448c04a47a152db71a7b34c20",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2583a54a88654cd9b10bfb367f8b232e",
              "IPY_MODEL_4e86a20e2b354092a09f2d277af6b970"
            ]
          }
        },
        "d2c305d448c04a47a152db71a7b34c20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2583a54a88654cd9b10bfb367f8b232e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_678ad20f818544f99041006778dfbdf2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc613f75db1943fb92294af1dd4742a8"
          }
        },
        "4e86a20e2b354092a09f2d277af6b970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_820bb65de0c7424690834f84b0d39cf9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 33436515.19it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d6471d57a34400f948e2526d80ce5d8"
          }
        },
        "678ad20f818544f99041006778dfbdf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc613f75db1943fb92294af1dd4742a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "820bb65de0c7424690834f84b0d39cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d6471d57a34400f948e2526d80ce5d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2kunal6/UniBonn/blob/master/cuda_lab/6/Assignment6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrurRTQrYaWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFqrklQZX9Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adapted from lab session and https://github.com/coolvision/vae_conv/blob/master/vae_conv_model_mnist.py\n",
        "# Simple Convolutional Autoencoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils as utils\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UQ0qvbOYSbe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "8847649eb0eb44fdab9258cb9ca179f1",
            "d2c305d448c04a47a152db71a7b34c20",
            "2583a54a88654cd9b10bfb367f8b232e",
            "4e86a20e2b354092a09f2d277af6b970",
            "678ad20f818544f99041006778dfbdf2",
            "fc613f75db1943fb92294af1dd4742a8",
            "820bb65de0c7424690834f84b0d39cf9",
            "1d6471d57a34400f948e2526d80ce5d8"
          ]
        },
        "outputId": "48bfb61c-fecd-4847-da1d-3e5d32c058a9"
      },
      "source": [
        "\n",
        "batch_size =100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Download Data\n",
        "\n",
        "cifar_train = dset.CIFAR10(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
        "cifar_test  = dset.CIFAR10(\"./\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
        "\n",
        "# Set Data Loader(input pipeline)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=cifar_train,batch_size=batch_size,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=cifar_train,batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8847649eb0eb44fdab9258cb9ca179f1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed6-yHvzY4b4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder \n",
        "# torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "#                 stride=1, padding=0, dilation=1,\n",
        "#                 groups=1, bias=True)\n",
        "# batch x 1 x 28 x 28 -> batch x 512\n",
        "\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "nc = 3\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            # input is (nc) x 28 x 28\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 14 x 14\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 7 x 7\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 4 x 4\n",
        "            nn.Conv2d(ndf * 4, 1024, 4, 1, 0, bias=False),\n",
        "            # nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(     1024, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2,     nc, 4, 2, 1, bias=False),\n",
        "            # nn.BatchNorm2d(ngf),\n",
        "            # nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            # nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "            # nn.Tanh()\n",
        "            nn.Sigmoid()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc21 = nn.Linear(512, z_dim)\n",
        "        self.fc22 = nn.Linear(512, z_dim)\n",
        "\n",
        "        self.fc3 = nn.Linear(z_dim, 512)\n",
        "        self.fc4 = nn.Linear(512, 1024)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU()\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def encode(self, x):\n",
        "        conv = self.encoder(x);\n",
        "        #print(\"encode conv\", conv.size())\n",
        "        h1 = self.fc1(conv.view(-1, 1024))\n",
        "        # print(\"encode h1\", h1.size())\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def decode(self, z):\n",
        "        #print(z.size())\n",
        "        h3 = self.relu(self.fc3(z))\n",
        "        deconv_input = self.fc4(h3)\n",
        "        #print(\"deconv_input\", deconv_input.size())\n",
        "        deconv_input = deconv_input.view(-1,1024,1,1)\n",
        "        #print(\"deconv_input\", deconv_input.size())\n",
        "        return self.decoder(deconv_input)\n",
        "\n",
        "    def reparametrize(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(\"x\", x.size())\n",
        "        mu, logvar = self.encode(x)\n",
        "        # print(\"mu, logvar\", mu.size(), logvar.size())\n",
        "        z = self.reparametrize(mu, logvar)\n",
        "        # print(\"z\", z.size())\n",
        "        #print(z.size())\n",
        "        decoded = self.decode(z)\n",
        "        # print(\"decoded\", decoded.size())\n",
        "        return decoded, mu, logvar\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm1t4ziLbDRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae = VAE(x_dim=3*32*32, h_dim1= 512, h_dim2=256, z_dim=20)\n",
        "if torch.cuda.is_available():\n",
        "    vae.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(vae.parameters())\n",
        "# return reconstruction error + KL divergence losses\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + KLD"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8z5DwxTcj8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train encoder and decoder\n",
        "# save and load model\n",
        "\n",
        "def train(epoch):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        image_n = torch.mul(data+0.25, 0.1 * torch.rand(batch_size,3,32,32))\n",
        "        if torch.cuda.is_available():\n",
        "            data = data.cuda()\n",
        "            image_n = image_n.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        recon_batch, mu, log_var = vae(image_n)\n",
        "        loss = loss_function(recon_batch, data, mu, log_var)\n",
        "        \n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFzIzz8X22WN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():    \n",
        "    vae.eval()\n",
        "    test_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                data = data.cuda()\n",
        "            recon, mu, log_var = vae(data)\n",
        "            \n",
        "            # sum up batch loss\n",
        "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
        "        \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfxUTChZsZvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8da8bc24-9ec9-4f6a-fb0d-79464b24de78"
      },
      "source": [
        "for epoch in range(1, 51):\n",
        "    train(epoch)\n",
        "    test()\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2724.283125\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tLoss: 2051.633438\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 2022.448750\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 2024.079219\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1982.463906\n",
            "====> Epoch: 1 Average loss: 4041252.2341\n",
            "====> Test set loss: 4856.3229\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1975.844531\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tLoss: 1968.099062\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 1935.393906\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tLoss: 1956.893125\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 1937.819062\n",
            "====> Epoch: 2 Average loss: 1965.0009\n",
            "====> Test set loss: 146857.0166\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1970.296094\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tLoss: 1947.793125\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 1957.479219\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tLoss: 1962.728125\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 1919.062031\n",
            "====> Epoch: 3 Average loss: 1950.6359\n",
            "====> Test set loss: 354135048.7715\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1936.259063\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tLoss: 1948.415937\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tLoss: 1895.488437\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tLoss: 1902.633750\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tLoss: 1934.442812\n",
            "====> Epoch: 4 Average loss: 1945.0444\n",
            "====> Test set loss: 269204.5648\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1933.981406\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tLoss: 1937.655000\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tLoss: 1936.038594\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tLoss: 1950.769063\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1915.146094\n",
            "====> Epoch: 5 Average loss: 1939.1127\n",
            "====> Test set loss: 8653446.8633\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1895.377187\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tLoss: 1953.285938\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tLoss: 1919.058594\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tLoss: 1937.656406\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tLoss: 1937.783281\n",
            "====> Epoch: 6 Average loss: 1931.9436\n",
            "====> Test set loss: 151151.2126\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1915.592969\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tLoss: 1922.227656\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tLoss: 1953.667500\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tLoss: 1903.317188\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tLoss: 1892.850156\n",
            "====> Epoch: 7 Average loss: 1927.0531\n",
            "====> Test set loss: 159748.2084\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1937.999844\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tLoss: 1894.114219\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tLoss: 1916.865469\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tLoss: 1938.430781\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tLoss: 1924.584063\n",
            "====> Epoch: 8 Average loss: 1924.4261\n",
            "====> Test set loss: 161272.6615\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1926.098906\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tLoss: 1975.921250\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tLoss: 1911.168594\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tLoss: 1913.017656\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tLoss: 1905.286094\n",
            "====> Epoch: 9 Average loss: 1919.9739\n",
            "====> Test set loss: 160158.1330\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1907.844531\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tLoss: 1908.083750\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tLoss: 1942.212500\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tLoss: 1921.670469\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1923.151719\n",
            "====> Epoch: 10 Average loss: 1917.9267\n",
            "====> Test set loss: 155696.6821\n",
            "Train Epoch: 11 [0/50000 (0%)]\tLoss: 1903.352187\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tLoss: 1912.974219\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tLoss: 1919.487187\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tLoss: 1906.002812\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tLoss: 1890.874063\n",
            "====> Epoch: 11 Average loss: 1915.5892\n",
            "====> Test set loss: 157937.4013\n",
            "Train Epoch: 12 [0/50000 (0%)]\tLoss: 1907.150312\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tLoss: 1902.010625\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tLoss: 1907.457656\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tLoss: 1931.337344\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tLoss: 1924.902656\n",
            "====> Epoch: 12 Average loss: 1913.7226\n",
            "====> Test set loss: 159299.7616\n",
            "Train Epoch: 13 [0/50000 (0%)]\tLoss: 1914.127812\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tLoss: 1932.726719\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tLoss: 1921.484062\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tLoss: 1971.958281\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tLoss: 1904.176250\n",
            "====> Epoch: 13 Average loss: 1913.1631\n",
            "====> Test set loss: 156674.2448\n",
            "Train Epoch: 14 [0/50000 (0%)]\tLoss: 1916.604063\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tLoss: 1914.810781\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tLoss: 1893.257188\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tLoss: 1910.540469\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tLoss: 1877.267500\n",
            "====> Epoch: 14 Average loss: 1909.0731\n",
            "====> Test set loss: 159016.5039\n",
            "Train Epoch: 15 [0/50000 (0%)]\tLoss: 1882.916250\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tLoss: 1927.658125\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tLoss: 1906.328438\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tLoss: 1905.976719\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tLoss: 1907.715469\n",
            "====> Epoch: 15 Average loss: 1907.1336\n",
            "====> Test set loss: 153686.8368\n",
            "Train Epoch: 16 [0/50000 (0%)]\tLoss: 1921.888125\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tLoss: 1889.462031\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tLoss: 1921.380156\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tLoss: 1927.210156\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tLoss: 1895.974062\n",
            "====> Epoch: 16 Average loss: 1904.2823\n",
            "====> Test set loss: 158458.0742\n",
            "Train Epoch: 17 [0/50000 (0%)]\tLoss: 1906.775938\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tLoss: 1904.417031\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tLoss: 1908.111562\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tLoss: 1900.151562\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tLoss: 1916.031719\n",
            "====> Epoch: 17 Average loss: 1902.0363\n",
            "====> Test set loss: 158725.3729\n",
            "Train Epoch: 18 [0/50000 (0%)]\tLoss: 1919.782813\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tLoss: 1925.146094\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tLoss: 1872.951406\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tLoss: 1891.304063\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tLoss: 1886.778594\n",
            "====> Epoch: 18 Average loss: 1899.4205\n",
            "====> Test set loss: 160293.2025\n",
            "Train Epoch: 19 [0/50000 (0%)]\tLoss: 1898.711094\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tLoss: 1882.993594\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tLoss: 1896.452969\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tLoss: 1944.246875\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tLoss: 1919.845625\n",
            "====> Epoch: 19 Average loss: 1897.7585\n",
            "====> Test set loss: 159229.1110\n",
            "Train Epoch: 20 [0/50000 (0%)]\tLoss: 1885.822969\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tLoss: 1909.705000\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tLoss: 1897.017031\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tLoss: 1901.093906\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tLoss: 1925.262813\n",
            "====> Epoch: 20 Average loss: 1895.2349\n",
            "====> Test set loss: 159682.5845\n",
            "Train Epoch: 21 [0/50000 (0%)]\tLoss: 1922.091875\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tLoss: 1923.466250\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tLoss: 1893.281563\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tLoss: 1907.840000\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tLoss: 1906.372656\n",
            "====> Epoch: 21 Average loss: 1892.2612\n",
            "====> Test set loss: 161379.0113\n",
            "Train Epoch: 22 [0/50000 (0%)]\tLoss: 1880.773906\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tLoss: 1912.504688\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tLoss: 1896.735625\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tLoss: 1900.089219\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tLoss: 1899.760313\n",
            "====> Epoch: 22 Average loss: 1890.5942\n",
            "====> Test set loss: 160817.0137\n",
            "Train Epoch: 23 [0/50000 (0%)]\tLoss: 1883.801094\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tLoss: 1875.905156\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tLoss: 1892.127344\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tLoss: 1887.284219\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tLoss: 1891.351719\n",
            "====> Epoch: 23 Average loss: 1887.8750\n",
            "====> Test set loss: 162099.0714\n",
            "Train Epoch: 24 [0/50000 (0%)]\tLoss: 1895.827031\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tLoss: 1861.207500\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tLoss: 1899.882656\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tLoss: 1887.023125\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tLoss: 1862.757188\n",
            "====> Epoch: 24 Average loss: 1885.4859\n",
            "====> Test set loss: 160753.3684\n",
            "Train Epoch: 25 [0/50000 (0%)]\tLoss: 1920.720469\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tLoss: 1872.758594\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tLoss: 1914.228594\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tLoss: 1919.257344\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tLoss: 1879.368438\n",
            "====> Epoch: 25 Average loss: 1882.6729\n",
            "====> Test set loss: 161219.8707\n",
            "Train Epoch: 26 [0/50000 (0%)]\tLoss: 1867.272813\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tLoss: 1891.781094\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tLoss: 1860.950000\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tLoss: 1879.546719\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tLoss: 1890.157656\n",
            "====> Epoch: 26 Average loss: 1880.2681\n",
            "====> Test set loss: 156301.0774\n",
            "Train Epoch: 27 [0/50000 (0%)]\tLoss: 1849.600938\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tLoss: 1852.766719\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tLoss: 1883.602969\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tLoss: 1907.377656\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tLoss: 1871.781875\n",
            "====> Epoch: 27 Average loss: 1877.8668\n",
            "====> Test set loss: 163242.7158\n",
            "Train Epoch: 28 [0/50000 (0%)]\tLoss: 1916.640156\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tLoss: 1857.595781\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tLoss: 1846.939844\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tLoss: 1858.933125\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tLoss: 1906.932031\n",
            "====> Epoch: 28 Average loss: 1875.5344\n",
            "====> Test set loss: 158898.9808\n",
            "Train Epoch: 29 [0/50000 (0%)]\tLoss: 1892.591563\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tLoss: 1899.100312\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tLoss: 1888.709219\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tLoss: 1892.231875\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tLoss: 1881.125156\n",
            "====> Epoch: 29 Average loss: 1874.8864\n",
            "====> Test set loss: 160836.1413\n",
            "Train Epoch: 30 [0/50000 (0%)]\tLoss: 1861.075625\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tLoss: 1891.706250\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tLoss: 1849.460938\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tLoss: 1830.536094\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tLoss: 1873.201562\n",
            "====> Epoch: 30 Average loss: 1872.3263\n",
            "====> Test set loss: 161444.5170\n",
            "Train Epoch: 31 [0/50000 (0%)]\tLoss: 1881.977187\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tLoss: 1868.661250\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tLoss: 1852.325781\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tLoss: 1860.900625\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tLoss: 1895.095625\n",
            "====> Epoch: 31 Average loss: 1870.3713\n",
            "====> Test set loss: 163620.6100\n",
            "Train Epoch: 32 [0/50000 (0%)]\tLoss: 1867.216563\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tLoss: 1930.755938\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tLoss: 1852.524375\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tLoss: 1873.312813\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tLoss: 1891.587188\n",
            "====> Epoch: 32 Average loss: 1869.5326\n",
            "====> Test set loss: 163354.7197\n",
            "Train Epoch: 33 [0/50000 (0%)]\tLoss: 1883.108906\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tLoss: 1839.515625\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tLoss: 1868.064219\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tLoss: 1847.394063\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tLoss: 1839.740781\n",
            "====> Epoch: 33 Average loss: 1867.3073\n",
            "====> Test set loss: 163635.8390\n",
            "Train Epoch: 34 [0/50000 (0%)]\tLoss: 1849.884687\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tLoss: 1869.173906\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tLoss: 1860.617969\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tLoss: 1872.239687\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tLoss: 1862.953906\n",
            "====> Epoch: 34 Average loss: 1865.5934\n",
            "====> Test set loss: 163776.4625\n",
            "Train Epoch: 35 [0/50000 (0%)]\tLoss: 1877.907187\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tLoss: 1854.891250\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tLoss: 1876.452969\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tLoss: 1869.088281\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tLoss: 1843.842500\n",
            "====> Epoch: 35 Average loss: 1863.9776\n",
            "====> Test set loss: 172873.8092\n",
            "Train Epoch: 36 [0/50000 (0%)]\tLoss: 1846.047969\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tLoss: 1879.511875\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tLoss: 1847.220313\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tLoss: 1879.778906\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tLoss: 1849.591250\n",
            "====> Epoch: 36 Average loss: 1863.1293\n",
            "====> Test set loss: 33463014.4533\n",
            "Train Epoch: 37 [0/50000 (0%)]\tLoss: 1837.524219\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tLoss: 1872.115625\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tLoss: 1871.953125\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tLoss: 1860.093437\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tLoss: 1904.608438\n",
            "====> Epoch: 37 Average loss: 1862.3046\n",
            "====> Test set loss: 825020744.7627\n",
            "Train Epoch: 38 [0/50000 (0%)]\tLoss: 1886.013438\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tLoss: 1869.889688\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tLoss: 1872.374219\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tLoss: 1865.771250\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tLoss: 1857.548750\n",
            "====> Epoch: 38 Average loss: 1861.3971\n",
            "====> Test set loss: 49728755.6845\n",
            "Train Epoch: 39 [0/50000 (0%)]\tLoss: 1861.664531\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tLoss: 1863.580937\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tLoss: 1828.815938\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tLoss: 1881.991406\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tLoss: 1851.268906\n",
            "====> Epoch: 39 Average loss: 1860.6584\n",
            "====> Test set loss: 42491770107.0800\n",
            "Train Epoch: 40 [0/50000 (0%)]\tLoss: 1879.496875\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tLoss: 1851.545938\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tLoss: 1854.314844\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tLoss: 1844.483438\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tLoss: 1857.878281\n",
            "====> Epoch: 40 Average loss: 1860.0822\n",
            "====> Test set loss: 13727560.7278\n",
            "Train Epoch: 41 [0/50000 (0%)]\tLoss: 1880.559531\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tLoss: 1891.151719\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tLoss: 1854.965156\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tLoss: 1846.454062\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tLoss: 1857.311094\n",
            "====> Epoch: 41 Average loss: 1859.7402\n",
            "====> Test set loss: 345978198621255.5000\n",
            "Train Epoch: 42 [0/50000 (0%)]\tLoss: 1891.992969\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tLoss: 1876.344844\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tLoss: 1870.537188\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tLoss: 1868.255156\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tLoss: 1848.395313\n",
            "====> Epoch: 42 Average loss: 1858.8905\n",
            "====> Test set loss: 6715181498619079426048.0000\n",
            "Train Epoch: 43 [0/50000 (0%)]\tLoss: 1837.225000\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tLoss: 1864.654219\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tLoss: 1880.489531\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tLoss: 1876.277656\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tLoss: 1841.179688\n",
            "====> Epoch: 43 Average loss: 1858.2766\n",
            "====> Test set loss: 147058060421705406349312.0000\n",
            "Train Epoch: 44 [0/50000 (0%)]\tLoss: 1847.371562\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tLoss: 1794.086875\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tLoss: 1847.858125\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tLoss: 1815.896719\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tLoss: 1824.340938\n",
            "====> Epoch: 44 Average loss: 1857.8992\n",
            "====> Test set loss: 13325845128434386206720.0000\n",
            "Train Epoch: 45 [0/50000 (0%)]\tLoss: 1870.954375\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tLoss: 1845.809688\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tLoss: 1842.328906\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tLoss: 1841.164687\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tLoss: 1876.550000\n",
            "====> Epoch: 45 Average loss: 1857.5365\n",
            "====> Test set loss: 96816150274447587422121330999296.0000\n",
            "Train Epoch: 46 [0/50000 (0%)]\tLoss: 1837.805469\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tLoss: 1858.000625\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tLoss: 1902.981406\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tLoss: 1859.859219\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tLoss: 1865.143125\n",
            "====> Epoch: 46 Average loss: 1856.8829\n",
            "====> Test set loss: inf\n",
            "Train Epoch: 47 [0/50000 (0%)]\tLoss: 1882.163125\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tLoss: 1860.764062\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tLoss: 1838.812031\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tLoss: 1892.468125\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tLoss: 1829.513594\n",
            "====> Epoch: 47 Average loss: 1856.3802\n",
            "====> Test set loss: inf\n",
            "Train Epoch: 48 [0/50000 (0%)]\tLoss: 1862.386719\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tLoss: 1875.931719\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tLoss: 1862.914844\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tLoss: 1853.644375\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tLoss: 1869.828906\n",
            "====> Epoch: 48 Average loss: 1855.6385\n",
            "====> Test set loss: inf\n",
            "Train Epoch: 49 [0/50000 (0%)]\tLoss: 1871.374063\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tLoss: 1860.947500\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tLoss: 1854.831719\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tLoss: 1856.606563\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tLoss: 1849.376250\n",
            "====> Epoch: 49 Average loss: 1855.4215\n",
            "====> Test set loss: 2937208505782328472562040832.0000\n",
            "Train Epoch: 50 [0/50000 (0%)]\tLoss: 1826.983281\n",
            "Train Epoch: 50 [10000/50000 (20%)]\tLoss: 1887.643125\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tLoss: 1860.506406\n",
            "Train Epoch: 50 [30000/50000 (60%)]\tLoss: 1845.171875\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tLoss: 1817.577188\n",
            "====> Epoch: 50 Average loss: 1854.8068\n",
            "====> Test set loss: 362152521756385321418752.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l0NltIXcnGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = 'drive/My Drive/colab_data/cuda_lab/6/vae.pth'\n",
        "torch.save(vae.state_dict(), PATH)\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgOKeoaIAAsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    \n",
        "vae = VAE(x_dim=3*32*32, h_dim1= 512, h_dim2=256, z_dim=2)\n",
        "vae.load_state_dict(torch.load(PATH))\n",
        "if torch.cuda.is_available():\n",
        "  vae.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gh7QTcwAwcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}